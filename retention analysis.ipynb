{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to confidentialityï¼Œthe notebook only contains some parts of codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.metrics import precision_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = pd.read_csv('path//RP_CLEANED.csv')\n",
    "print(rp.shape)\n",
    "rp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Function to calculate the missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "\n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "\n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "\n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "        # Print some summary information\n",
    "        print (\"Dataframe Rows =  \" + str(df.shape[0]) + \"\\n\" + \"Dataframe Columns =  \" + str(df.shape[1]) + \"\\n\" +\n",
    "            \"Columns with Null Values =  \" + str(mis_val_table_ren_columns.shape[0]))\n",
    "\n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get % of missing values for the columns with missing values.\n",
    "missing_values_table(rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type to datetime for further processing\n",
    "rp['REGISTRATION_DATE'] = pd.to_datetime(rp['REGISTRATION_DATE'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Make sure all data are from 2016 to 2020\n",
    "rp = rp[rp['REGISTRATION_DATE'].map(lambda x: x > datetime.date(2015,12,31))]\n",
    "\n",
    "# reorganize registration date by ascending order for each customer\n",
    "rp = rp.loc[:].sort_values(['EMAIL_ADDRESS_ID','REGISTRATION_DATE'], ascending=True)\n",
    "rp.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert birth date column to caculatable type for creating age column.\n",
    "rp['BIRTH_DATE'] = pd.to_datetime(rp['BIRTH_DATE'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Create age column\n",
    "rp[\"age\"] = pd.to_datetime('2020-10-09 00:00:00') - rp['BIRTH_DATE']\n",
    "\n",
    "# Covert age column data type to integer.\n",
    "rp['age'] = rp['age'].dt.days.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because measurement of age value is in days, so we need to convert days to years\n",
    "rp['age'] = (rp['age'] / 365).round(decimals=0)\n",
    "\n",
    "# Take a look at how age distribute\n",
    "print(\"AGE <15:\", rp[rp[\"age\"] < 15].shape[0])\n",
    "print(\"AGE >=100 :\", rp[rp[\"age\"] >= 80].shape[0])\n",
    "\n",
    "sns.boxplot(x=rp[\"age\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from boxplot of age, we can create a age group to clearly identify how customers distribute\n",
    "bins = [0, 14, 24, 49, 64, 90, 1000]\n",
    "labels = ['children', 'youth', 'young adult', 'senior adult', 'seniors', 'extreme old']\n",
    "rp['age_group'] = pd.cut(rp['age'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check top 10 category of products in sale.\n",
    "cate = rp.copy()\n",
    "cate = pd.DataFrame(cate['GSCM_Prod_Name_sf'].value_counts().iloc[:10])\n",
    "cate = cate.reset_index()\n",
    "cate = cate.rename(columns={'GSCM_Prod_Name_sf': 'qty', 'index': 'category'})\n",
    "\n",
    "ay = sns.barplot(y='category', x='qty', data=cate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check top 10 most popular colors.\n",
    "color = rp.copy()\n",
    "color = pd.DataFrame(color['Attb12_gscm'].value_counts().iloc[:10])\n",
    "color = color.reset_index()\n",
    "color = color.rename(columns={'Attb12_gscm': 'qty', 'index': 'color'})\n",
    "\n",
    "axy = sns.barplot(y='color', x='qty', data=color);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a columns called color to differentiate popular colors from minor colors\n",
    "rp['color'] = np.where(rp['Attb12_gscm'].isin(['BLACK', 'ORCHID GRAY', 'PURPLE', 'SILVER', 'WHITE']), \\\n",
    "                        rp['Attb12_gscm'], 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out bset sell hhp models in RP table.\n",
    "cat = rp.copy()\n",
    "hhp = cat[cat['GSCM_Prod_Name_sf'] == 'HHP']['Attb09_gscm'].value_counts()\n",
    "\n",
    "#Prepare dataframe for further chat plotted\n",
    "hhp = pd.DataFrame(hhp)\n",
    "hhp = hhp.reset_index().rename({'index': 'hhp_models'})\n",
    "\n",
    "# Plot the chart.\n",
    "hhp = hhp.rename(columns={'index': 'hhp_models', 'Attb09_gscm': 'qty'})\n",
    "ax = sns.barplot(y='hhp_models', x='qty', data=hhp.iloc[:15]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a dataframe to create new column called number of registration\n",
    "tp = rp.groupby('EMAIL_ADDRESS_ID')['GSCM_Prod_Name_sf'].count()\n",
    "cpt = pd.DataFrame(data=tp)\n",
    "cpt.reset_index(inplace=True)\n",
    "\n",
    "# Merge two tables to add in one column\n",
    "rp = rp.merge(cpt, on='EMAIL_ADDRESS_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column name to correct one\n",
    "rp = rp.rename(columns={'GSCM_Prod_Name_sf_y':'Number_of_registration'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to get lastest registration date for each customer\n",
    "idx = rp['EMAIL_ADDRESS_ID'].drop_duplicates(keep='last').index\n",
    "i = rp.loc[idx]\n",
    "\n",
    "# to get number of days of last purchase of product\n",
    "#i['num_of_days_since_last_pro_purchase'] = pd.to_datetime('2020-10-09 00:00:00') - i['REGISTRATION_DATE']\n",
    "\n",
    "# #Get the new column of ''num_of_days_since_last_pro_purchase''\n",
    "# ii = i[['EMAIL_ADDRESS_ID', 'num_of_days_since_last_pro_purchase']]\n",
    "# rp = rp.merge(ii, on='EMAIL_ADDRESS_ID', how='left')\n",
    "\n",
    "# # convert data tpe from timedelta to integert.\n",
    "# rp['num_of_days_since_last_pro_purchase'] = rp['num_of_days_since_last_pro_purchase'].dt.days.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column called FSA in order to adding buying capacity further\n",
    "rp['FSA'] = rp['PostalCodeArea'].map(lambda x: x[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if lable is imbalanced and result shows we need to use sampling tool to samplt the data.\n",
    "df['Churn'].value_counts()\n",
    "print('Percentage of Churn is ', df[df['Churn'] == 1]['Churn'].count() / df[['Churn']].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0', 'City_PC', 'Prov_PC', 'age', 'since_last_open', 'FSA'])\n",
    "# Redundant column generated when merging tables, drop it.\n",
    "\n",
    "df = df.drop(columns=['EMAIL_ADDRESS_ID', 'REGISTRATION_DATE', 'GSCM_Prod_Name_sf_x', 'LastOpenedDate'])\n",
    "# Drop Categorical columns which does not help model building.\n",
    "\n",
    "# Made categorical columns switch to readable numerical columns for the models\n",
    "df = pd.get_dummies(columns=['age_group', 'color', 'models'], data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have missing values before build model.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation Matrix\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(30, 25)\n",
    "sns.heatmap(df.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model XGBoost\n",
    "X = df.drop(columns=['Churn']) # all features\n",
    "y = df['Churn'] # label\n",
    "# Splited data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# In order to avoid data leakage, imputed missing values of buying capacity.\n",
    "mean_bc = X_train['buy_capacity'].mean()\n",
    "# Filling NaN value with average of buying capacity for training and testing sets.\n",
    "X_train = X_train.fillna(mean_bc)\n",
    "X_test = X_test.fillna(mean_bc)\n",
    "\n",
    "os = SMOTE()\n",
    "X_train, y_train = os.fit_resample(X_train, y_train)\n",
    "\n",
    "dt = XGBClassifier() # Applying estimator - DecisionTree classifier.\n",
    "dt.fit(X_train, y_train) # Fitted the model with Training set.\n",
    "y_pred = dt.predict(X_test) # Made prediction for test set by applying trained decisionTree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = dt.predict(X_train)\n",
    "print(classification_report(y_train_pred, y_train))\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Printed the report of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dt.feature_importances_, index=X.columns).sort_values().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As feature importance graph shoed above, OPT_OUT, Open and Clicks play important roles in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers - median of each columns will not be influenced significantly compared to average\n",
    "def remove_outliers(x):\n",
    "# Definite a function to replace outliers with medians of each column in training set\n",
    "    X_train[x] = np.where(X_train[x] >= X_train[x].quantile(0.95), X_train[x].quantile(0.95), X_train[x])\n",
    "    \n",
    "# Replace outliers in test set with medians of each column in training set\n",
    "    X_test[x] = np.where(X_test[x] >= X_train[x].quantile(0.95), X_train[x].quantile(0.95), X_test[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Churn']) # Created all features\n",
    "y = df['Churn'] # Created label\n",
    "# Splited data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Have a list of features that need to be handled outliers\n",
    "lst = ['Number_of_registration', 'Open', 'Clicks']\n",
    "for i in lst:\n",
    "    remove_outliers(i)\n",
    "    \n",
    "# In order to avoid data leakage, imputed missing values of buying capacity.\n",
    "mean_bc = X_train['buy_capacity'].mean()\n",
    "# Filling NaN value with average of buying capacity for training and testing sets.\n",
    "X_train = X_train.fillna(mean_bc)\n",
    "X_test = X_test.fillna(mean_bc)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "\n",
    "ratio_lr = 0.5\n",
    "os = SMOTE(sampling_strategy=ratio_lr)\n",
    "X_train_os, y_train_os = os.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "param = {\n",
    "    #'penalty':['l1', 'l2']ï¼Œ\n",
    "    'C': [1, 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(class_weight={0: ratio_lr, 1: 1 - ratio_lr})\n",
    "gs_lr = GridSearchCV(lr, param, cv=3, scoring='f1', n_jobs=-1, verbose=2)\n",
    "gs_lr.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best estimator and get predict proba for each prediction.\n",
    "gsbm_lr = gs_lr.best_estimator_\n",
    "y_train_pred_proba = gsbm_lr.predict_proba(X_train_scaled)[:, 1]\n",
    "y_pred_proba = gsbm_lr.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Set up threshold for determine \"1\" and \"0\" in order to tune the model.\n",
    "thre = 0.7\n",
    "y_train_pred_proba_lr = list(map(lambda x: 1 if x >= thre else 0, y_train_pred_proba))\n",
    "y_pred_proba_lr = list(map(lambda x: 1 if x >= thre else 0, y_pred_proba))\n",
    "\n",
    "print(classification_report(y_train, y_train_pred_proba_lr))\n",
    "print(classification_report(y_test, y_pred_proba_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check different threholds affecting the results\n",
    "print(pd.Series(y_pred_proba > 0.5).value_counts())\n",
    "print(pd.Series(y_pred_proba > 0.8).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "\n",
    "def plot_roc(y_train, y_test, y_pred_prob, y_train_pred_prob):\n",
    "    # Plot a ROC curve\n",
    "    plt.figure(1, figsize=(8, 8))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    \n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_prob)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_prob)\n",
    "    plt.plot(fpr_train, tpr_train, label='Training set')\n",
    "    plt.plot(fpr_test, tpr_test, label='Test set')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(y_train, y_test, y_pred_proba, y_train_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTree with scaler\n",
    "\n",
    "ratio_dt = 0.5\n",
    "os = SMOTE(sampling_strategy=ratio_dt)\n",
    "X_train_os, y_train_os = os.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "param = {\n",
    "    #'criterion':['gini', 'entropy'],\n",
    "    'max_depth': [10,20,30,40] \n",
    "}\n",
    "dt = DecisionTreeClassifier()\n",
    "gs_dt = GridSearchCV(dt, param, cv=3, scoring='f1', n_jobs=-1, verbose=2)\n",
    "gs_dt.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best estimator and get predict proba for each prediction.\n",
    "gsbm_dt = gs_dt.best_estimator_\n",
    "y_train_pred_proba = gsbm_dt.predict_proba(X_train_scaled)[:, 1]\n",
    "y_pred_proba = gsbm_dt.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Set up threshold for determine \"1\" and \"0\" in order to tune the model.\n",
    "thre = 0.5\n",
    "y_train_pred_proba_dt = list(map(lambda x: 1 if x >= thre else 0, y_train_pred_proba))\n",
    "y_pred_proba_dt = list(map(lambda x: 1 if x >= thre else 0, y_pred_proba))\n",
    "\n",
    "print(classification_report(y_train, y_train_pred_proba_dt))\n",
    "print(classification_report(y_test, y_pred_proba_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest with scaler\n",
    "\n",
    "ratio_rf = 0.5\n",
    "os = SMOTE(sampling_strategy=ratio_rf)\n",
    "X_train_os, y_train_os = os.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "param = {\n",
    "    #'criterion':['gini', 'entropy'],\n",
    "    'max_depth':[10,20,30,40],\n",
    "    'n_estimators':[100, 150]\n",
    "    #'min_samples_split':[2,3,4,5,8,10],\n",
    "    #'max_features':['sqrt', 'log2']\n",
    "}\n",
    "rf = RandomForestClassifier()\n",
    "gs_rf = GridSearchCV(rf, param, cv=3, scoring='f1', n_jobs=-1, verbose=2)\n",
    "gs_rf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsbm_rf = gs_rf.best_estimator_\n",
    "y_train_pred_proba = gsbm_rf.predict_proba(X_train_scaled)[:, 1]\n",
    "y_pred_proba = gsbm_rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Set up threshold for determine \"1\" and \"0\" in order to tune the model.\n",
    "thre = 0.8\n",
    "y_train_pred_proba_rf = list(map(lambda x: 1 if x >= thre else 0, y_train_pred_proba))\n",
    "y_pred_proba_rf = list(map(lambda x: 1 if x >= thre else 0, y_pred_proba))\n",
    "\n",
    "print(classification_report(y_train, y_train_pred_proba_rf))\n",
    "print(classification_report(y_test, y_pred_proba_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "ratio_xg = 0.5\n",
    "os = SMOTE(sampling_strategy=ratio_xg)\n",
    "X_train_os, y_train_os = os.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "param = {\n",
    "    'max_depth ':[10, 15],\n",
    "    'learning_rate ':[0.1]\n",
    "}\n",
    "\n",
    "xg = XGBClassifier()\n",
    "\n",
    "gs_xg = GridSearchCV(xg, param, cv=3, scoring='f1', n_jobs=-1, verbose=2)\n",
    "gs_xg.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsbm_xg = gs_xg.best_estimator_\n",
    "y_train_pred_proba = gsbm_xg.predict_proba(X_train_scaled)[:, 1]\n",
    "y_pred_proba = gsbm_xg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Set up threshold for determine \"1\" and \"0\" in order to tune the model.\n",
    "thre = 0.6\n",
    "y_train_pred_proba_xg = list(map(lambda x: 1 if x >= thre else 0, y_train_pred_proba))\n",
    "y_pred_proba_xg = list(map(lambda x: 1 if x >= thre else 0, y_pred_proba))\n",
    "\n",
    "print(classification_report(y_train, y_train_pred_proba_xg))\n",
    "print(classification_report(y_test, y_pred_proba_xg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier and Keras Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network - MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os = SMOTE()\n",
    "X_train_os, y_train_os = os.fit_resample(X_train_scaled, y_train)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10, 10), max_iter=1000)\n",
    "mlp.fit(X_train_os, y_train_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mlp = mlp.predict(X_test_scaled)\n",
    "print(classification_report(y_test, pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Sequential()\n",
    "seq.add(Dense(12, input_dim=29, activation='relu'))\n",
    "seq.add(Dense(9, activation='relu'))\n",
    "seq.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "seq.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "seq.fit(X_train_scaled, y_train, epochs=15, batch_size=10)\n",
    "\n",
    "pred_seq = seq.predict_classes(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, pred_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train_scaled.shape)\n",
    "pred_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC_curve for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Churn']) \n",
    "y = df['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "lst = ['Number_of_registration', 'Open', 'Clicks']\n",
    "for i in lst:\n",
    "    remove_outliers(i)\n",
    "\n",
    "mean_bc = X_train['buy_capacity'].mean()\n",
    "X_train = X_train.fillna(mean_bc)\n",
    "X_test = X_test.fillna(mean_bc)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "os = SMOTE(sampling_strategy=0.5)\n",
    "X_train_os, y_train_os = os.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "models = [\n",
    "    {\n",
    "    'label': 'Logistic Regression',\n",
    "    'model': LogisticRegression(C=1)\n",
    "    },\n",
    "    {'label': 'Random Forest',\n",
    "     'model': RandomForestClassifier(max_depth=20, n_estimators=150)\n",
    "    },\n",
    "    {'label': 'Decision Tree',\n",
    "     'model': DecisionTreeClassifier(max_depth=20)},\n",
    "    {'label': 'XGBoost',\n",
    "     'model': XGBClassifier(learning_rate=0.1, max_depth=10)},\n",
    "    {'label': 'K-Nearest Neighbor',\n",
    "     'model': KNeighborsClassifier(weights='uniform', n_neighbors=5, algorithm='auto')},\n",
    "    {'label': 'Gradient Boosting',\n",
    "     'model': GradientBoostingClassifier(learning_rate=0.1)\n",
    "    }\n",
    "]\n",
    "for i in models:\n",
    "    model = i['model']\n",
    "    model.fit(X_train_os, y_train_os)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test_scaled)[:,1])\n",
    "    auc = metrics.roc_auc_score(y_test, model.predict(X_test_scaled))\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area=%0.2f)' % (i['label'], auc))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('sensitivity(true Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
